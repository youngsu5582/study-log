> 링크 : https://www.youtube.com/watch?v=O3fnPkX5ybc

리뷰 플랫폼의 적용 포인트

- 맛집 & 미용실 등 방문자리뷰
- 공연 / 전시 관람평
- 네이버 여행 - 해외명소 & 호텔 리뷰

등등

시간이 지나며
호출량과 필터링 등 복잡한 요구사항이 늘어났다.

그럼에도, 빠른 시간에도 응답을 해줘야한다.

리뷰가 적은 업체는 크게 문제가 되지 않지만, 수천,수만 건이라면?

## 초기 대응 - DB 인덱스

`자주 조회되는 필터 + 정렬 패턴` 으로 DB 인덱스를 추가해 대응
-> 늘어나는 조회 조건에 비례해 인덱스가 많아짐

리뷰 컬렉션에 인덱스가 도합 30개가 걸리더라!

![500](https://i.imgur.com/6m1Eb05.png)

### 한계

- 인덱스 추가 / 제거 요청이 너무 잦아짐

`조회` - `정렬` 이 multiply 로 생겨남
( 조회 4개 / 정렬 2개 일 시, 정렬필터가 하나 더 생기면 4개의 인덱스 추가 생성 )

DBA 입장에선 다수 인덱스 거는데 많은 리소스 소모
( 16개 인덱스 drop + 새로 거는데 24시간 소요된 사례 )

- 쓰기 속도 감소

insert 하나만 해도 30개의 B+ Tree 에 올라가는 무거운 작업 발생

## Elasticsearch 도입

몽고 DB 는 주로 쓰기 담당, 조회용 DB 는 조회 담당

미리 정의되지 않은 다양한 조회조건의 요청에 빠른 응답 필요
사내 앱으로 제공되거나 팀에서 운용경험이 있는 DB ( 원활하게 플랫폼 적용 및 관리하기 위해서 )

3가지 선택지가 있었다.

1. 몽고 DB 스케일 아웃 ( scale-out 자원 확보 후, 리뷰 복제 컬렉션 만들어서 새 패턴 인덱스 )
-> 메인 DB 이므로 scale-out 비용이 비싸고, 같은 인덱스를 계속 거는 연장선이므로 근본적 해결 X

2. 검색엔진 사용 ( 검색에 특화된 저장소를 통해 DB 인덱스가 커버 못하는 조회를 대신 처리 )
-> 쓰기 시점부터 각 값들이 조회에 최적화되어 저장되므로 다양한 조회조건에 유연하게 응답

3. Redis 캐시 ( 첫 번째 조회 이후 동일 조회 들어올 경우 캐시된 결과 응답 )
-> 응답속도는 가장 빠르나 조회요청이 다양하게 들어오므로 캐시 히트율이 낮고, 과도한 메모리 사용 예상

=> Elasticsearch 로 결정!

POST 요청을 왼쪽과 같이 보내면 오른쪽과 같은 결과 반환

![](https://i.imgur.com/EDjtOPD.png)

![](https://i.imgur.com/5Z5H72d.png)

일반적인 DB는 내용을 그대로 테이블 구조로 저장

전통적인 RDBMS 는 `like` 검색을 사용하므로 데이터가 늘어날수록 검색 해야할 대상이 늘어나 시간이 오래 걸린다.
=> 역인덱스의 필요성

![](https://i.imgur.com/G3ORUs0.png)

fox 를 포함한 단어를 바로 가져올 수 있다.

![](https://i.imgur.com/CEbzAsk.png)

![](https://i.imgur.com/oIhAa2P.png)

특정 칼럼을 키워드 타입으로 매핑하면, 자동으로 만들어준다.

![](https://i.imgur.com/KuNWSJC.png)

![](https://i.imgur.com/r3wrIcS.png)

텍스트 타입으로 매핑하면 적절히 쪼개서 저장한다.

=> 따로 인덱스를 걸지 않아도 조회에 최적화

### 고도화된 검색니즈 대응 가능

구현이 어려운 텍스트 내 검색 니즈도 대응이 가능하다.

- 리뷰텍스트 내 특정 단어 검색 : 특정 단어가 포함된 리뷰 검색

- 특정 컨셉 리뷰어 검색 : 특정 단어 포함된 리뷰어 검색

### 단점

역인덱스는 항상 생성비용을 생각해야 한다

- 별도 설정하지 않으면 모든 필드에 역인덱스가 걸린다.
-> 필드 개수가 많을수록 역인덱스 생성에 상당한 비용이 들고, 읽기 성능을 저하시킬 수 있음
=> 꼭 필요한 필드만 매핑시키거나 검색조건에 포함되지 않는 필드는 `{index:false}`

- 필드는 검색 용도에 맞게 효율적으로 매핑해야 한다.
문자열은 검색용도 따라 keyword or text, depth 구조는 검색용도 따라 object or nested

타 DB 대비 리소스를 많이 점유한다.
-> 일괄 대체가 아닌 점진적 대체로, 모니터링 및 투입비율을 늘려가며 해결 가능

write 직후 검색 불가 ( refresh_interval 필요 )
-> 초 단위 delay 는 서비스에서 용인 가능

## 도입과정 및 결과

발급받은 ES 자원이 전체 리소스 부하 감당하기에 부족 ( CPU 8, Memory 32GB 노드 4대 ), 최대한 효율적으로 활용
-> 조회에 꼭 필요한 필드만 매핑 ( 조회필터 & 정렬조건 총 33개 필드 ES 리뷰인덱스에 매핑 )

ES에 1차 리뷰조회해 리뷰 id 얻고, DB에 2차로 해당 id 로 쿼리해 전체 리뷰정보 얻음

- 쓰기 작업

![](https://i.imgur.com/xB3tvT4.png)

카프카 통해 CDC 발행 -> 카프카 컨슈머가 리뷰 BULK INSERT
( 카프카로 DB 변경 이벤트를 받아 ES에 sync )

- 읽기 작업

![](https://i.imgur.com/5hMYUJ9.png)

ES 에서 id 까지만 뽑고, 해당 id 들로 몽고DB에 가져오게 했음

- 읽기 작업 2

![](https://i.imgur.com/fBwylsm.png)

DB <-> ES 중간 산출물이 있으므로, 해당 값 캐싱해 성능 향상 가능

- 리뷰 count : 이미 캐시 활용했으므로, 조회대상 DB만 ES 로 변경

### CQRS 관점

read, write 는 구조적으로 다양한 면에서 차이가 남 ( 요청량, 비즈니스 로직 포함여부, validation 필요여부, 캐시 도움 등등 )
두 모델이 분리되면 유지보수성이 증가한다.
필요에 따라 단순 쿼리 분리, 물리적 저장소 분리 등 다양한 형태로 적용가능

### 점진적 도입

전체 트래픽이 부족해서, 점진적 + 부분적 전환을 해야했다.

기존 DB가 커버하는 요소는 바로 돌리지 않아도 됐으므로, 다음 두 케이스 중 하나에 해당하는 경우 조회 분산을 했다.

DB 인덱스가 커버하지 못하는
- case1: 특정 서비스 조회요청 인 경우
- case2: 특정 조회조건이 조회요청에 포함된 경우

-> 이 조건들은 고정되지 않고 계속 바뀔 수 있는 값

소스코드에 하드코딩 하면, api 배포를 하지 않으면 변경이 불가능해 효율 및 안정성을 떨어뜨린다.

- case1 : ES 지표 모니터링하며 실시간으로 서비스/조회조건을 ES에 투입 or 제거하고 싶은데, 매번 배포 나가야함
- case2 : ES 장애상황시, 조회트래픽 DB로 빼야하는데, 전체 api 배포를 해야 하므로 서비스 downtime 증가

=> feature toggle 을 활용!
#### feature toggle

코드에 변화를 주지 않고도 기능 변경이 가능
요청이 설정값을 참조할 수 있는 형태면 다양한 방식 구현 가능(DB,캐시 등 설정값 넣고 분기 가능)

타 저장소 참조를 최소화 하기 위해, feature toggle 적용 시 커스텀 HTTP 헤더를 넣어주는 방식
(`authorization` + `ES-전환-서비스-id들` or `ES-전환-조회필터들`)

#### 자동 fallback 시스템

ES 부하가 커질 경우, ES로 가는 트래픽 자동 제거되도록 설정
서비스 입장에선 조금 더 느린 응답을 받더라도 장애상황은 전파 X

![](https://i.imgur.com/6y3R8Sd.png)

이슈 발생해도 개발자 개입 없이 안정적 서빙 가능

## ES 도입결과

DB 인덱스가 커버하는 경우는 큰 차이 없으나
조회조건 복잡하게 한 경우, 도입 이전 대비 확실히 줄어듬 ( 2.67 -> 0.28초 )

사업자 쪽 서비스에서 복잡한 조건의 리뷰조회요청
EX) 코스트코 상봉점에 작년에 달린 리뷰들을 추천점수 낮은순 조회

-> DB 인덱스를 잘 못타는 특정 조회 slow query 가 99.7% 감소
-> 새로 생긴 조회필터도 인덱스 없이 안정적 리뷰조회 / 통계집회 가능

## ES 운영 tips & Opensearch 전환

### string 구조 매핑, keyword vs text

string 저장하는 두 가지 방법: keyword vs text

- keyword 타입 : string 통째로 역인덱스 구조 저장
- text 타입 : string 데이터를 `analyzer` 를 거쳐 역인덱스 구조로 저장

![](https://i.imgur.com/BGMaT3v.png)

꼭 텍스트 검색이 필요한게 아니면 keyword 타입으로 매핑하자.
인덱싱 시 analyzing 과정 생략

### depth 구조 매핑, object vs nested

- object 타입 : depth 구조 내 데이터를 각각 매핑으로서 역인덱스 구조 데이터 저장
- nested 타입 : depth 구조 내 데이터를 각각 개별 doc 으로서 역인덱스 구조 데이터 저장

![700](https://i.imgur.com/kK7D6UO.png)

object 는 통째로 저장, nested 는 개별 저장

- nested 는 배열 사이즈가 커지면, read/write 에 따른 오버헤드 증가

`이동경로.행정동:신도림동, 이동경로.방문일시:2020-04-07~` 쿼리시
둘다 결과에 포함된다. ( 올바른 결과 )

`이동경로.행정동:신도림동, 이동경로.방문일시:2020-04-11~` 쿼리시
object 는 결과에 포함된다. ( 잘못된 결과 ), nested 는 결과에 포함되지 않는다.

- depth 구조가 배열 형태
- depth 구조 내 두 개 이상 필드에 AND 조건 쿼리 될 경우
-> 이때는 nested 타입으로 매핑하자.

그 외의 경우, object 타입으로 매핑을 권장

### `_` id 필드 별도 매핑

`doc_id` 값을 정렬 들에 활용한다면, 같은 값을 복사해 별도 keyword 타입으로 매핑하는 걸 권장

- ES의 모든 타입엔 doc_values 옵션이 true, 이 경우 파일시스템 캐시를 통해 정렬 / 집계 등이 원활
- `_id` 는 내부 필드이므로 doc_values 설정이 불가능, 정렬 시 메모리 사용량 급증
- 메모리에 올리고, flush 되지 않아 요청차단도 발생 가능

![](https://i.imgur.com/8PkQYew.png)

### 샤드 설정

ES 인덱스 내 저장공간 단위는 샤드
샤드당 크기는 20~40GB 권장 ( 더 잘게 쪼개보니 오히려 느렸음 )

primary 외 replica 샤드도 1개 이상 두는걸 추천 ( index.number_of_replica )
운영 중 특정 노드 다운되어도 장애 없이 운용

![](https://i.imgur.com/zaB3h6g.png)

1번 노드가 장애나도 3,4번 노드의 0,2번 샤드가 각각 primary 로 승격해 다운타임 없이 요청 처리 가능

> replica 는 몇개?
> replica 늘릴수록 read 는 빨라지고, write 는 느려짐, 더 많은 스토리지 차지 ( 당연 )

### `_routing` 설정

샤드 배치는 doc 의 `_routing` 값이 결정

지정해주지 않으면 기본적으로 인덱스 내 모든 샤드에 `broadcast query` 를 보냄.
요청 하나가 샤드 하나에만 가게 하려면, `_routing` 을 write / read 시 모두 커스텀하게 지정해주는 걸 권장 ( 2배 이상 요청량 증가 )

## Opensearch

Opensearch ??

ElasticSearch 가 오픈소스 라이센스가 아닌 SSPL 라이센스로 바꾸면서, 클라우드 제공업체들이 서비스를 만들어서 제공하기 어렵게 만들었다.
-> 커뮤니티에서 ES 7.10 을 포크떠서 거의 동일한 기능을 제공하게 만들었다.

Opensearch 는 라이센스 문제 때문에 ElasticSearch 의 상용 기능을 사용할 수 없음
-> OpenTelemetry 나, 커뮤니티의 기반의 다른 오픈소스 플러그인을 붙이는 식

ElasticSearch 는 유료 구독 모델화 ( 머신러닝, 벡터화 등 제공 )

=> 결국, 앞으로 계속 방향이 달라질 것

- 보안 기능은 OpenSearch 만 무료 제공
- elasticsearch 가 일반 검색 40%,140% 더 빠르다고 주장 ( 벡터 검색은 더 빠를수도..! )
-> 실제 결과는 다를수도 있음
- Opensearch 는 AWS 에서 일정량 프리티어로 제공

오히려, OpenSearch 를 통해서 노드를 줄였는데 CPU 사용량이 줄고, 쿼리 응답속도 역시 더욱 빨라졌다.
